---
title: "Lab 13: Logistic Regression"
author: "Name here"
output: pdf_document
---


For this question, we will use a historical data set with NCAA tournament results. However, we will now look at predicting a binary outcome, win or loss by the higher seeded team.

```{r, message = F}
library(rjags)
library(knitr)
library(tidyverse)
ncaa <- read_csv('https://raw.githubusercontent.com/stat456/labs/main/Lab12_data.csv') %>%
  filter(Seed.Diff != 0) %>%
  mutate(Seed.Diff = -1 * Seed.Diff,
         SAG.Diff = -1 * SAG.Diff,
         upset = as.numeric(Result == 'Loss'))

ncaa_train <- ncaa %>% filter(Season < 2020)
ncaa_test <- ncaa %>% filter(Season >= 2020)
```

#### 1. (4 points)

Create two figures to explore the impact of `Seed.Diff` and `SAG.Diff` on `upset` Add a smoother line to approximate the relationship.

```{r, fig.cap = 'The higher seed would, unsurprisingly, be expected to win. We do see some potential evidence of non-linearity and non-zero upset probability.'}
ncaa %>% ggplot(aes(y = upset, x = Seed.Diff)) + 
  geom_jitter(alpha = .2, height = .05, width = .3) + theme_bw() +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  ggtitle('Upsets vs. Seed difference from historical NCAA basketball')
```

```{r, fig.cap = 'The negative values correspond to lower seeded teams having higher Sagarin ratings. In general the Sagarin ratings seems to do a good job of estimating upset probability - even when ratings do not agree with seeds.'}
ncaa %>% ggplot(aes(y = upset, x = SAG.Diff)) + 
  geom_jitter(alpha = .2, height = .05, width = .3) + theme_bw() +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  ggtitle('Upset vs. Sagarin difference from historical NCAA basketball')

```


#### 2. (4 points)
Write a short caption to accompany each figure.

Added above

#### 3. (4 points)
Deviance Information Criteria (DIC) is a Bayesian analog to AIC. Consider the two model below, which do you prefer and why?

```{r}
# Model String
logistic = "model {
  for ( i in 1:N ) {
    y[i] ~ dbern(p[i])
    logit(p[i]) = beta0 + beta1 * x[i]
  }
  beta0 ~ dnorm(M0, 1 / S0^2)
  beta1 ~ dnorm(M1, 1 / S1^2)
} "
writeLines( logistic, con='logistic.txt')
```

```{r}
# Runs JAGS Model: Seeds
seeds_model <- jags.model( file = "logistic.txt", 
                           data =  list(y = ncaa_train$upset, 
                                        x = ncaa_train$Seed.Diff,
                                        N = nrow(ncaa_train), 
                                        M0 = 0, 
                                        S0 = .001,
                                        M1 = 0, 
                                        S1 = 3),
                         n.chains = 2, n.adapt = 5000)
update(seeds_model, n.iter = 5000)

seeds_coda <- coda.samples(seeds_model, 
                          variable.names = c('beta0','beta1'), 
                          n.iter = 5000)

summary(seeds_coda)

summary(glm(upset ~ Seed.Diff -1, family = binomial, data = ncaa_train))

dic_seed <- dic.samples(seeds_model, 5000)
dic_seed
```

```{r}
# Runs JAGS Model: Sagarin
sag_model <- jags.model( file = "logistic.txt", 
                           data =  list(y = ncaa_train$upset, 
                                        x = ncaa_train$SAG.Diff,
                                        N = nrow(ncaa_train), 
                                        M0 = 0, 
                                        S0 = .001,
                                        M1 = 0, 
                                        S1 = 3),
                         n.chains = 2, n.adapt = 5000)
update(sag_model, n.iter = 5000)

sag_coda <- coda.samples(sag_model, 
                          variable.names = c('beta0','beta1'), 
                          n.iter = 5000)

summary(sag_coda)

dic_sag <- dic.samples(sag_model, 5000)
dic_sag
```

```{r}
diffdic(dic_seed, dic_sag)
```

**_DIC suggests that the Sagarin model is superior._**

#### 4. (4 points)

Fit the best possible model using DIC as your criteria. You may want to consider interactions and/or non-linearity terms.

__omitted__

#### 5. (4 points)

Write out the formal model you've selected in part 4, including all priors.

\begin{eqnarray}
  upset_i &\sim Bernoulli(p_i)\\
  logit(p_i) =\ \beta_0 + \beta_1 * x_{sag diff} \\
  \beta_0 &\sim N(0, .001^2) \\
  \beta_1 &\sim N(0, 3^2)\\
\end{eqnarray}


#### 6. (4 points)

Interpret your parameters in the model and summarize your findings. Assume you are telling your parents how statistics can be useful for filling out an NCAA bracket.

__Using the Sagarin ratings, the probability of an upset is related to the difference in ratings between the two teams. Unsurprisingly, the larger the difference in ratings the lower the probability of an upset. If your goal is to fill out a bracket, then picking the team with the better Sagarin rating would be a good strategy, but still expect upsets to happen.__

#### 7. (4 points)

Construct a posterior predictive distribution to calculate the winning probability of the following games, Note your model is likely set to predict the point spread for the higher seed:

1. Montana State (Seed: 14, Sagarin: 133) vs. Kansas State (Seed: 3, Sagarin: 18)
2. Purdue (Seed: 1, Sagarin: 9) vs. Farleigh Dickinson (Seed: 16, Sagarin: 310)
3. Arizona (Seed: 2, Sagarin: 10) vs. Princeton (Seed 15:, Sagarin: 118)
4. Connecticut (Seed: 4, Sagarin: 4) vs. Iona (Seed: 13, Sagarin: 86)

```{r}
# Model String
logistic_pp = "model {
  for ( i in 1:N ) {
    y[i] ~ dbern(p[i])
    logit(p[i]) = beta0 + beta1 * x[i]
  }
  beta0 ~ dnorm(M0, 1 / S0^2)
  beta1 ~ dnorm(M1, 1 / S1^2)
  pp_msu ~ dbern(ilogit(beta0 + beta1 * 115))
  pp_purdue ~ dbern(ilogit(beta0 + beta1 * 301))
  pp_arizona ~ dbern(ilogit(beta0 + beta1 * 108))
  pp_uconn ~ dbern(ilogit(beta0 + beta1 * 82))
} "
writeLines( logistic_pp, con='logistic_pp.txt')

```


```{r}
# Runs JAGS Model: Sagarin
sag_model_pp <- jags.model( file = "logistic_pp.txt", 
                           data =  list(y = ncaa_train$upset, 
                                        x = ncaa_train$SAG.Diff,
                                        N = nrow(ncaa_train), 
                                        M0 = 0, 
                                        S0 = .001,
                                        M1 = 0, 
                                        S1 = 3),
                         n.chains = 2, n.adapt = 5000)

update(sag_model_pp, n.iter = 1000)

sag_coda_pp <- coda.samples(sag_model_pp, 
                          variable.names = c('beta0','beta1',
                                             'pp_msu','pp_purdue','pp_arizona','pp_uconn'), 
                          n.iter = 5000)

pp_out <- tibble(vals = c(sag_coda_pp[[1]][,'pp_msu'],
                          sag_coda_pp[[1]][,'pp_purdue'],
                          sag_coda_pp[[1]][,'pp_arizona'],
                          sag_coda_pp[[1]][,'pp_uconn']),
                 type = rep(c('Kansas St', 
                              'Purdue', 
                              'Arizona',
                              'UConn'),
                            each = 5000))

pp_out %>% group_by(type) %>% 
  summarize(`upset prob` = mean(vals == 1)) %>%
  kable(digits = 4)
```

#### 8. (1 EC point)

Rather than using DIC as a model selection criteria, compare your model's ability to predict outcomes during the 2021 and 2022 tournaments (`ncca_test`).

Consider using classification error (win/loss) and Brier score (`brier.scoer()` in the `iterativeBMA` package. )
